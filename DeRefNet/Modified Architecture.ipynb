{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INZGVNLDdlNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46229bb1-9aff-409c-c4e8-7f0e6382e0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPU memory growth enabled\n",
            " Mixed precision policy: <DTypePolicy \"mixed_float16\">\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, mixed_precision\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, AveragePooling2D,\n",
        "                                     GlobalAveragePooling2D, BatchNormalization,\n",
        "                                     Activation, Add, Concatenate, Dense, Dropout,\n",
        "                                     Multiply, Reshape)\n",
        "import os, kagglehub\n",
        "\n",
        "# Disable XLA JIT (to avoid huge pinned‚Äêhost allocs)\n",
        "tf.config.optimizer.set_jit(False)\n",
        "\n",
        "# GPU memory growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    print(\" GPU memory growth enabled\")\n",
        "else:\n",
        "    print(\" No GPU detected\")\n",
        "\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\" Mixed precision policy:\", mixed_precision.global_policy())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"nafishamoin/bangladeshi-crops-disease-dataset\")\n",
        "print(\" Raw dataset path:\", path)\n",
        "\n",
        "main_folder = os.listdir(path)[0]\n",
        "main_path = os.path.join(path, main_folder)\n",
        "print(\" Main folder:\", main_folder)\n",
        "print(\"Contents of main folder:\", os.listdir(main_path)[:5])\n",
        "\n",
        "image_root = os.path.join(main_path, \"Crop___DIsease\")\n",
        "print(\" Image root folder:\", image_root)\n",
        "\n",
        "classes = os.listdir(image_root)\n",
        "print(\" Classes found:\", classes[:10])"
      ],
      "metadata": {
        "id": "vqse1rLodza3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3fa39d-2c7b-433e-de11-dfd7aea055af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Raw dataset path: /kaggle/input/bangladeshi-crops-disease-dataset\n",
            " Main folder: CropDisease\n",
            "Contents of main folder: ['Crop___DIsease']\n",
            " Image root folder: /kaggle/input/bangladeshi-crops-disease-dataset/CropDisease/Crop___DIsease\n",
            " Classes found: ['Wheat___Yellow_Rust', 'Potato___Healthy', 'Corn___Healthy', 'Wheat___Brown_Rust', 'Corn___Gray_Leaf_Spot', 'Rice___Brown_Spot', 'Wheat___Healthy', 'Rice___Leaf_Blast', 'Potato___Late_Blight', 'Rice___Healthy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = os.path.join(main_path, \"Crop___DIsease\")"
      ],
      "metadata": {
        "id": "QJu80Ii7OEqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load datasets with one hot labels\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 8\n",
        "img_size   = (160, 160)\n",
        "\n",
        "dataset_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_root,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "dataset_val = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_root,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "print(\"Train element spec:\", dataset_train.element_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErQIhnFUkxX2",
        "outputId": "598e9d9c-d6d3-4314-abc8-a6c7c50de48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31053 files belonging to 15 classes.\n",
            "Using 24843 files for training.\n",
            "Found 31053 files belonging to 15 classes.\n",
            "Using 6210 files for validation.\n",
            "Train element spec: (TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 15), dtype=tf.float32, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = dataset_train.element_spec[1].shape[-1]\n",
        "print(\"Detected num_classes =\", num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdfklRjLMh1S",
        "outputId": "a5c9f831-6ae7-4556-96e7-6e40a8b0bc3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected num_classes = 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_ds = (\n",
        "    dataset_train\n",
        "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    .shuffle(500)\n",
        "    .prefetch(1)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    dataset_val\n",
        "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    .prefetch(1)\n",
        ")"
      ],
      "metadata": {
        "id": "MiRc6greeROK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow.keras import layers, regularizers, backend as K, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, Activation,\n",
        "    MaxPooling2D, AveragePooling2D,\n",
        "    GlobalAveragePooling2D, Dense, Dropout, Concatenate,Reshape, Dot, Add, Lambda\n",
        ")"
      ],
      "metadata": {
        "id": "N-Yl9rP7H0rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rsa_attention(x, reg_l2=1e-4, reg_l1=1e-4, name=None):\n",
        "    C = x.shape[-1]\n",
        "    H = x.shape[1]\n",
        "    W = x.shape[2]\n",
        "    N = H * W\n",
        "\n",
        "    f = Conv2D(C,1,padding='same',use_bias=True,\n",
        "               kernel_regularizer=regularizers.l2(reg_l2),\n",
        "               bias_regularizer   =regularizers.l1(reg_l1),\n",
        "               name=(name+\"_f\" if name else None))(x)\n",
        "    g = Conv2D(C,1,padding='same',use_bias=True,\n",
        "               kernel_regularizer=regularizers.l2(reg_l2),\n",
        "               bias_regularizer   =regularizers.l1(reg_l1),\n",
        "               name=(name+\"_g\" if name else None))(x)\n",
        "    h = Conv2D(C,1,padding='same',use_bias=True,\n",
        "               kernel_regularizer=regularizers.l2(reg_l2),\n",
        "               bias_regularizer   =regularizers.l1(reg_l1),\n",
        "               name=(name+\"_h\" if name else None))(x)\n",
        "\n",
        "    f_flat = Reshape((N, C), name=(name+\"_reshape_f\" if name else None))(f)\n",
        "    g_flat = Reshape((N, C), name=(name+\"_reshape_g\" if name else None))(g)\n",
        "    h_flat = Reshape((N, C), name=(name+\"_reshape_h\" if name else None))(h)\n",
        "\n",
        "    scores = Dot(axes=(2,2), name=(name+\"_score\" if name else None))([f_flat, g_flat])\n",
        "\n",
        "    inv_sqrt_C = 1.0 / math.sqrt(C)\n",
        "    scores = Lambda(lambda t: t * inv_sqrt_C,\n",
        "                    name=(name+\"_scale\" if name else \"rsa_scale\"))(scores)\n",
        "\n",
        "    alpha = Activation('softmax', name=(name+\"_softmax\" if name else \"rsa_softmax\"))(scores)\n",
        "    o_flat = Dot(axes=(2,1), name=(name+\"_apply\" if name else \"rsa_apply\"))([alpha, h_flat])\n",
        "    o = Reshape((H, W, C), name=(name+\"_out\" if name else \"rsa_out\"))(o_flat)\n",
        "    return o"
      ],
      "metadata": {
        "id": "CM7tZKLvHg6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model building blocks\n",
        "\n",
        "def residual_block(x, filters, stride=1):\n",
        "    shortcut = x\n",
        "    x = Conv2D(filters,3,strides=stride,padding='same',use_bias=False)(x)\n",
        "    x = BatchNormalization()(x); x=Activation('relu')(x)\n",
        "    x = Conv2D(filters,3,padding='same',use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if stride!=1 or shortcut.shape[-1]!=filters:\n",
        "        shortcut = Conv2D(filters,1,strides=stride,use_bias=False)(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    x = Add()([x,shortcut]); return Activation('relu')(x)\n",
        "\n",
        "def residual_block_group(x, f, n, stride=1):\n",
        "    x = residual_block(x,f,stride)\n",
        "    for _ in range(1,n):\n",
        "        x = residual_block(x,f,1)\n",
        "    return x\n",
        "\n",
        "def dense_block(x, layers_n, g_rate):\n",
        "    feats = [x]\n",
        "    for _ in range(layers_n):\n",
        "        y = Concatenate()(feats)\n",
        "        y = BatchNormalization()(y); y=Activation('relu')(y)\n",
        "        y = Conv2D(4*g_rate,1,padding='same',use_bias=False)(y)\n",
        "        y = BatchNormalization()(y); y=Activation('relu')(y)\n",
        "        y = Conv2D(g_rate,3,padding='same',use_bias=False)(y)\n",
        "        feats.append(y)\n",
        "    return Concatenate()(feats)\n",
        "\n",
        "def transition_layer(x):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(x.shape[-1]//2,1,padding='same',use_bias=False)(x)\n",
        "    return AveragePooling2D(2,2)(x)\n"
      ],
      "metadata": {
        "id": "TF1z-F13hbza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import callbacks, optimizers, losses\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "def build_rsa_derefnet(input_shape=(160,160,3), num_classes=15, growth_rate=12):\n",
        "    i = Input(shape=input_shape)\n",
        "\n",
        "    # Initial conv\n",
        "    x0 = Conv2D(32,3,padding='same',use_bias=False)(i)\n",
        "    x0 = BatchNormalization()(x0); x0 = Activation('relu')(x0)\n",
        "\n",
        "    # Residual stream with RSA\n",
        "    r1 = residual_block_group(x0,  64, 1, stride=1)\n",
        "    p1 = AveragePooling2D(pool_size=4)(r1)\n",
        "    a1 = rsa_attention(p1, name=\"res1\")\n",
        "    r2 = residual_block_group(a1, 128, 1, stride=2)\n",
        "    p2 = AveragePooling2D(pool_size=2)(r2)\n",
        "    a2 = rsa_attention(p2, name=\"res2\")\n",
        "    r3 = residual_block_group(a2, 256, 1, stride=2)\n",
        "    g_r = GlobalAveragePooling2D()(r3)\n",
        "\n",
        "    # Dense stream with RSA\n",
        "    d1 = Conv2D(32,3,padding='same',use_bias=False)(i)\n",
        "    d1 = BatchNormalization()(d1); d1 = Activation('relu')(d1)\n",
        "    d1 = MaxPooling2D(2)(d1)\n",
        "    db1 = dense_block(d1, 3, growth_rate)\n",
        "    t1 = transition_layer(db1)\n",
        "    a3 = rsa_attention(t1, name=\"dens1\")\n",
        "    db2 = dense_block(a3, 3, growth_rate)\n",
        "    t2 = transition_layer(db2)\n",
        "    a4 = rsa_attention(t2, name=\"dens2\")\n",
        "    db3 = dense_block(a4, 5, growth_rate)\n",
        "    g_d = GlobalAveragePooling2D()(db3)\n",
        "\n",
        "    # Fuse & classify\n",
        "    f = Concatenate()([g_r, g_d])\n",
        "    f = Dropout(0.5)(f)\n",
        "    out = Dense(num_classes, activation='softmax', dtype='float32')(f)\n",
        "\n",
        "    return Model(i, out)\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_schedule = optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_rsa_derefnet(\n",
        "    input_shape=(160,160,3),\n",
        "    num_classes=num_classes,\n",
        "    growth_rate=12\n",
        ")\n",
        "from tensorflow.keras import optimizers, losses\n",
        "\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "total_steps = steps_per_epoch * 30\n",
        "\n",
        "cosine = optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=3e-4, decay_steps=total_steps\n",
        ")\n",
        "\n",
        "opt = optimizers.AdamW(learning_rate=cosine, weight_decay=1e-4, clipnorm=1.0)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=30,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK88othpZYOh",
        "outputId": "6c109f3d-041d-4f73-cef6-bc3999395ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 85ms/step - accuracy: 0.6626 - loss: 1.5954 - val_accuracy: 0.8095 - val_loss: 1.1222\n",
            "Epoch 2/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 62ms/step - accuracy: 0.8775 - loss: 0.9790 - val_accuracy: 0.8662 - val_loss: 0.9938\n",
            "Epoch 3/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 62ms/step - accuracy: 0.9072 - loss: 0.8942 - val_accuracy: 0.9045 - val_loss: 0.8378\n",
            "Epoch 4/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 63ms/step - accuracy: 0.9193 - loss: 0.8550 - val_accuracy: 0.9233 - val_loss: 0.8166\n",
            "Epoch 5/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 62ms/step - accuracy: 0.9294 - loss: 0.8227 - val_accuracy: 0.9242 - val_loss: 0.7913\n",
            "Epoch 6/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 63ms/step - accuracy: 0.9363 - loss: 0.7956 - val_accuracy: 0.9188 - val_loss: 0.7957\n",
            "Epoch 7/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 62ms/step - accuracy: 0.9416 - loss: 0.7740 - val_accuracy: 0.9477 - val_loss: 0.7415\n",
            "Epoch 8/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 63ms/step - accuracy: 0.9436 - loss: 0.7644 - val_accuracy: 0.9485 - val_loss: 0.7456\n",
            "Epoch 9/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 60ms/step - accuracy: 0.9446 - loss: 0.7491 - val_accuracy: 0.9530 - val_loss: 0.7016\n",
            "Epoch 10/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 61ms/step - accuracy: 0.9499 - loss: 0.7372 - val_accuracy: 0.9617 - val_loss: 0.6862\n",
            "Epoch 11/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 61ms/step - accuracy: 0.9555 - loss: 0.7260 - val_accuracy: 0.9599 - val_loss: 0.6963\n",
            "Epoch 12/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 61ms/step - accuracy: 0.9561 - loss: 0.7157 - val_accuracy: 0.9501 - val_loss: 0.7121\n",
            "Epoch 13/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 61ms/step - accuracy: 0.9582 - loss: 0.7092 - val_accuracy: 0.9475 - val_loss: 0.7038\n",
            "Epoch 14/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 61ms/step - accuracy: 0.9622 - loss: 0.7023 - val_accuracy: 0.9274 - val_loss: 0.7767\n",
            "Epoch 15/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 61ms/step - accuracy: 0.9647 - loss: 0.6937 - val_accuracy: 0.9699 - val_loss: 0.6647\n",
            "Epoch 16/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 61ms/step - accuracy: 0.9688 - loss: 0.6860 - val_accuracy: 0.9697 - val_loss: 0.6617\n",
            "Epoch 17/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 65ms/step - accuracy: 0.9686 - loss: 0.6802 - val_accuracy: 0.9654 - val_loss: 0.6707\n",
            "Epoch 18/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 62ms/step - accuracy: 0.9714 - loss: 0.6771 - val_accuracy: 0.9699 - val_loss: 0.6638\n",
            "Epoch 19/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 60ms/step - accuracy: 0.9723 - loss: 0.6703 - val_accuracy: 0.9713 - val_loss: 0.6613\n",
            "Epoch 20/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 61ms/step - accuracy: 0.9731 - loss: 0.6649 - val_accuracy: 0.9662 - val_loss: 0.6630\n",
            "Epoch 21/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 62ms/step - accuracy: 0.9764 - loss: 0.6578 - val_accuracy: 0.9726 - val_loss: 0.6568\n",
            "Epoch 22/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 61ms/step - accuracy: 0.9798 - loss: 0.6521 - val_accuracy: 0.9738 - val_loss: 0.6506\n",
            "Epoch 23/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 62ms/step - accuracy: 0.9794 - loss: 0.6513 - val_accuracy: 0.9734 - val_loss: 0.6526\n",
            "Epoch 24/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 63ms/step - accuracy: 0.9798 - loss: 0.6479 - val_accuracy: 0.9747 - val_loss: 0.6489\n",
            "Epoch 25/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 62ms/step - accuracy: 0.9836 - loss: 0.6410 - val_accuracy: 0.9744 - val_loss: 0.6491\n",
            "Epoch 26/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 62ms/step - accuracy: 0.9819 - loss: 0.6439 - val_accuracy: 0.9744 - val_loss: 0.6490\n",
            "Epoch 27/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 63ms/step - accuracy: 0.9844 - loss: 0.6395 - val_accuracy: 0.9734 - val_loss: 0.6474\n",
            "Epoch 28/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 63ms/step - accuracy: 0.9845 - loss: 0.6381 - val_accuracy: 0.9749 - val_loss: 0.6482\n",
            "Epoch 29/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 63ms/step - accuracy: 0.9840 - loss: 0.6370 - val_accuracy: 0.9747 - val_loss: 0.6479\n",
            "Epoch 30/30\n",
            "\u001b[1m3106/3106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 63ms/step - accuracy: 0.9842 - loss: 0.6371 - val_accuracy: 0.9742 - val_loss: 0.6482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot training and validation metrics\n",
        "def plot_training_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'go-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'go-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)\n"
      ],
      "metadata": {
        "id": "wXCB62QM4Hry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5jpsgvkrC-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}